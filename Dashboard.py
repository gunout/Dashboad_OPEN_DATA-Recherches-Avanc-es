import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import requests
import json
from datetime import datetime, timedelta, timezone
import numpy as np
import feedparser
import concurrent.futures
import time
import warnings
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import re
import urllib.parse
from bs4 import BeautifulSoup
import io
import base64

warnings.filterwarnings('ignore')

# =============================================================================
# CONFIGURATION DE LA PAGE
# =============================================================================
st.set_page_config(
    page_title="Dashboard Analytics Data.gouv.fr - 24K Datasets",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================================
# STYLES CSS PERSONNALIS√âS - TH√àME BLEU BLANC ROUGE
# =============================================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem; 
        color: #1f77b4; 
        text-align: center; 
        margin-bottom: 1rem;
        font-weight: bold;
        background: linear-gradient(90deg, #0055A4 0%, #FFFFFF 50%, #EF4135 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .section-header {
        font-size: 1.8rem; 
        color: #0055A4; 
        margin: 1.5rem 0 1rem 0; 
        border-bottom: 3px solid #EF4135; 
        padding-bottom: 0.5rem;
        font-weight: 600;
    }
    .metric-card {
        background: linear-gradient(135deg, #FFFFFF 0%, #F0F8FF 100%); 
        padding: 1.2rem; 
        border-radius: 10px; 
        margin: 0.8rem 0;
        border-left: 4px solid #0055A4;
        color: #000000;
        box-shadow: 0 2px 4px rgba(0,85,164,0.1);
    }
    .alert-red {
        background-color: #FFEBEE; 
        padding: 1rem; 
        border-radius: 5px; 
        border-left: 5px solid #EF4135; 
        margin: 0.5rem 0;
        color: #000000;
    }
    .alert-orange {
        background-color: #FFF3E0; 
        padding: 1rem; 
        border-radius: 5px; 
        border-left: 5px solid #FF9800; 
        margin: 0.5rem 0;
        color: #000000;
    }
    .alert-green {
        background-color: #E8F5E8; 
        padding: 1rem; 
        border-radius: 5px; 
        border-left: 5px solid #4CAF50; 
        margin: 0.5rem 0;
        color: #000000;
    }
    .filter-section {
        background: linear-gradient(135deg, #F8F9FF 0%, #E8F4FF 100%); 
        padding: 1.5rem; 
        border-radius: 10px; 
        margin-bottom: 1rem;
        color: #000000;
        border: 2px solid #0055A4;
    }
    .kpi-card {
        background: linear-gradient(135deg, #0055A4 0%, #1f77b4 100%); 
        color: white; 
        padding: 1.2rem; 
        border-radius: 10px; 
        margin: 0.3rem;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0,85,164,0.2);
    }
    .data-source {
        font-size: 0.8rem; 
        color: #666; 
        font-style: italic;
        text-align: center;
    }
    .rss-item {
        border-left: 4px solid #EF4135; 
        padding-left: 15px; 
        margin: 10px 0;
        color: #000000;
        background: #FFFFFF;
        padding: 15px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(239,65,53,0.1);
    }
    .progress-info {
        color: #0055A4; 
        font-weight: bold;
    }
    .dataset-card {
        border: 2px solid #0055A4; 
        padding: 15px; 
        margin: 10px 0; 
        border-radius: 8px; 
        background: linear-gradient(135deg, #FFFFFF 0%, #F0F8FF 100%);
        color: #000000;
        box-shadow: 0 2px 4px rgba(0,85,164,0.1);
    }
    .search-result {
        border-left: 5px solid #0055A4; 
        padding: 20px; 
        margin: 15px 0; 
        background: linear-gradient(135deg, #FFFFFF 0%, #F8F9FF 100%);
        color: #000000;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,85,164,0.1);
        border-right: 1px solid #EF4135;
        border-bottom: 1px solid #EF4135;
    }
    .stats-grid {
        display: grid; 
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); 
        gap: 10px; 
        margin: 15px 0;
    }
    .stat-item {
        background: linear-gradient(135deg, #FFFFFF 0%, #F0F8FF 100%); 
        padding: 15px; 
        border-radius: 8px; 
        box-shadow: 0 2px 4px rgba(0,0,0,0.1); 
        text-align: center;
        color: #000000;
        border: 1px solid #0055A4;
    }
    .result-item {
        background: #FFFFFF;
        border: 2px solid #0055A4;
        border-radius: 10px;
        padding: 20px;
        margin: 15px 0;
        box-shadow: 0 4px 6px rgba(0,85,164,0.1);
        color: #000000;
        border-top: 3px solid #EF4135;
    }
    .result-title {
        font-size: 1.3rem;
        font-weight: bold;
        color: #0055A4;
        margin-bottom: 10px;
        border-bottom: 1px solid #EF4135;
        padding-bottom: 8px;
    }
    .result-meta {
        color: #0055A4;
        font-size: 1rem;
        margin-bottom: 8px;
        font-weight: 500;
    }
    .result-stats {
        background: linear-gradient(135deg, #F8F9FF 0%, #E8F4FF 100%);
        padding: 12px;
        border-radius: 6px;
        margin: 12px 0;
        color: #000000;
        border: 1px solid #0055A4;
    }
    
    /* Styles pour les boutons */
    .stButton > button {
        background: linear-gradient(135deg, #0055A4 0%, #1f77b4 100%);
        color: white;
        border: none;
        border-radius: 6px;
        font-weight: 600;
    }
    .stButton > button:hover {
        background: linear-gradient(135deg, #004494 0%, #1866a3 100%);
        color: white;
    }
    
    /* Style pour les s√©lecteurs */
    .stSelectbox, .stTextInput, .stMultiselect, .stSlider {
        color: #000000;
    }
    
    /* Style pour les dataframes */
    .dataframe {
        color: #000000 !important;
    }
    
    /* Style pour les onglets */
    .stTabs [data-baseweb="tab-list"] {
        gap: 2px;
        background-color: #F0F8FF;
        padding: 10px;
        border-radius: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background: linear-gradient(135deg, #FFFFFF 0%, #F0F8FF 100%);
        border-radius: 6px;
        gap: 1px;
        padding-top: 10px;
        padding-bottom: 10px;
        font-weight: 600;
        color: #0055A4;
        border: 1px solid #0055A4;
    }
    .stTabs [data-baseweb="tab"][aria-selected="true"] {
        background: linear-gradient(135deg, #0055A4 0%, #1f77b4 100%);
        color: white;
    }
    
    /* Style pour les m√©triques */
    [data-testid="stMetricValue"] {
        color: #0055A4;
        font-weight: bold;
    }
    [data-testid="stMetricLabel"] {
        color: #EF4135;
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)

# =============================================================================
# CONFIGURATION ET CONSTANTES
# =============================================================================
API_BASE_URL = "https://www.data.gouv.fr/api/1"
DATASETS_URL = f"{API_BASE_URL}/datasets/"
ORGANIZATIONS_URL = f"{API_BASE_URL}/organizations/"
MAX_DATASETS = 24187
CONCURRENT_REQUESTS = 5
PAGE_SIZE = 20
CACHE_TTL = 1800

# Flux RSS √©conomiques
RSS_FEEDS = [
    "https://www.economie.gouv.fr/rss",
    "https://www.insee.fr/rss/actualites.xml",
    "https://www.banque-france.fr/rss",
    "https://www.impots.gouv.fr/rss",
    "https://www.entreprises.gouv.fr/rss"
]

# Cat√©gories th√©matiques pour une recherche plus efficace
THEMATIQUES = {
    "√©conomie": ["√©conomie", "finances", "PIB", "croissance", "inflation", "commerce"],
    "entreprises": ["entreprises", "PME", "startup", "innovation", "industrie"],
    "emploi": ["emploi", "ch√¥mage", "travail", "salaires", "m√©tiers"],
    "budget": ["budget", "d√©penses", "recettes", "d√©ficit", "dette"],
    "commerce": ["commerce", "export", "import", "douane", "√©changes"],
    "√©nergie": ["√©nergie", "√©lectricit√©", "p√©trole", "gaz", "renouvelable"],
    "transport": ["transport", "logistique", "infrastructure", "mobilit√©"],
    "environnement": ["environnement", "climat", "pollution", "d√©veloppement durable"],
    "sant√©": ["sant√©", "m√©dical", "h√¥pital", "m√©decine", "pharmaceutique"],
    "√©ducation": ["√©ducation", "enseignement", "universit√©", "formation"]
}

# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================
def safe_get(dictionary, keys, default=''):
    """S√©curise l'acc√®s aux dictionnaires imbriqu√©s"""
    for key in keys:
        if isinstance(dictionary, dict) and key in dictionary:
            dictionary = dictionary[key]
        else:
            return default
    return dictionary

def convertir_date(date_str):
    """Convertit une string de date en datetime avec timezone"""
    if not date_str:
        return None
    
    try:
        if 'Z' in date_str:
            date_str = date_str.replace('Z', '+00:00')
        
        dt = datetime.fromisoformat(date_str)
        
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        
        return dt
    except Exception as e:
        return None

def est_recent(date_creation, jours=30):
    """V√©rifie si une date est r√©cente (dans les X derniers jours)"""
    if not date_creation:
        return False
    
    maintenant = datetime.now(timezone.utc)
    
    if date_creation.tzinfo is None:
        date_creation = date_creation.replace(tzinfo=timezone.utc)
    
    return (maintenant - date_creation) < timedelta(days=jours)

# =============================================================================
# CLIENT API DATA.GOUV.FR AM√âLIOR√â
# =============================================================================
class DataGouvAPIClient:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8'
        })
    
    @st.cache_data(ttl=CACHE_TTL, show_spinner="Chargement des datasets...")
    def rechercher_datasets_avances(_self, query="", page=1, page_size=20, organization=None, format_type=None, tags=None):
        """Recherche avanc√©e optimis√©e pour les 24K datasets"""
        params = {
            'page': page,
            'page_size': page_size,
            'sort': '-created'
        }
        
        if query:
            params['q'] = query
        
        if organization and organization != "Toutes les organisations":
            params['organization'] = organization
        
        # Construction des filtres complexes
        filters = []
        if format_type and format_type != "Tous les formats":
            filters.append(f'format:{format_type}')
        
        if tags:
            for tag in tags:
                filters.append(f'tag:{tag}')
        
        if filters:
            params['q'] = f"{query} {' '.join(filters)}" if query else ' '.join(filters)
        
        try:
            response = _self.session.get(DATASETS_URL, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                return data
            elif response.status_code == 429:
                st.warning("‚ö†Ô∏è Trop de requ√™tes. Attente avant nouvel essai...")
                time.sleep(2)
                response = _self.session.get(DATASETS_URL, params=params, timeout=60)
                if response.status_code == 200:
                    return response.json()
                else:
                    st.error(f"‚ùå Erreur API apr√®s attente: {response.status_code}")
                    return {'data': [], 'total': 0, 'page': page, 'page_size': page_size}
            else:
                st.error(f"‚ùå Erreur API: {response.status_code}")
                return {'data': [], 'total': 0, 'page': page, 'page_size': page_size}
                
        except requests.exceptions.Timeout:
            st.error("‚è∞ Timeout: L'API met trop de temps √† r√©pondre")
            return {'data': [], 'total': 0, 'page': page, 'page_size': page_size}
        except Exception as e:
            st.error(f"üö® Erreur inattendue: {str(e)}")
            return {'data': [], 'total': 0, 'page': page, 'page_size': page_size}
    
    @st.cache_data(ttl=CACHE_TTL)
    def get_datasets_populaires(_self, limit=50):
        """R√©cup√®re les datasets les plus populaires avec cache et gestion d'erreurs am√©lior√©e"""
        try:
            # Essayer d'abord par followers
            params = {
                'page_size': limit,
                'sort': '-metrics.followers'
            }
            
            response = _self.session.get(DATASETS_URL, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                datasets = data.get('data', [])
                if datasets:
                    return datasets
            
            # Fallback: tri par vues
            st.info("Chargement des datasets par popularit√© (fallback)...")
            params = {
                'page_size': limit,
                'sort': '-metrics.views'
            }
            
            response = _self.session.get(DATASETS_URL, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                datasets = data.get('data', [])
                if datasets:
                    return datasets
            
            # Dernier fallback: datasets r√©cents
            params = {
                'page_size': limit,
                'sort': '-created'
            }
            
            response = _self.session.get(DATASETS_URL, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('data', [])
            else:
                return []
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur chargement datasets populaires: {str(e)}")
            return []
    
    @st.cache_data(ttl=CACHE_TTL)
    def get_organizations_list(_self, limit=100):
        """R√©cup√®re la liste des organisations avec cache"""
        try:
            params = {
                'page_size': limit,
                'sort': '-datasets'
            }
            
            response = _self.session.get(ORGANIZATIONS_URL, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('data', [])
            else:
                return []
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur chargement organisations: {str(e)}")
            return []
    
    @st.cache_data(ttl=CACHE_TTL)
    def get_dataset_details(_self, dataset_id):
        """R√©cup√®re les d√©tails d'un dataset sp√©cifique avec cache"""
        url = f"{DATASETS_URL}{dataset_id}/"
        try:
            response = _self.session.get(url, timeout=60)
            if response.status_code == 200:
                return response.json()
            else:
                return None
        except Exception as e:
            return None
    
    def get_resource_data(self, resource_url):
        """T√©l√©charge et parse les donn√©es d'une ressource"""
        try:
            if resource_url.endswith('.csv'):
                response = self.session.get(resource_url, timeout=60)
                response.raise_for_status()
                return pd.read_csv(io.StringIO(response.text))
            elif resource_url.endswith(('.xlsx', '.xls')):
                response = self.session.get(resource_url, timeout=60)
                response.raise_for_status()
                return pd.read_excel(response.content)
            elif resource_url.endswith('.json'):
                response = self.session.get(resource_url, timeout=60)
                response.raise_for_status()
                data = response.json()
                if isinstance(data, list):
                    return pd.DataFrame(data)
                else:
                    return pd.DataFrame([data])
            else:
                st.warning(f"Format non support√©: {resource_url}")
                return None
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur chargement ressource {resource_url}: {str(e)}")
            return None

    @st.cache_data(ttl=3600)
    def get_datasets_stats(_self):
        """R√©cup√®re les statistiques globales sur les datasets avec √©chantillonnage"""
        try:
            # R√©cup√©rer plusieurs pages pour avoir un bon √©chantillon
            all_datasets = []
            total_datasets = 0
            
            # Premi√®re requ√™te pour obtenir le total
            params = {'page_size': 1, 'page': 1}
            response = _self.session.get(DATASETS_URL, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                total_datasets = data.get('total', 0)
            
            # R√©cup√©rer un √©chantillon repr√©sentatif (3 pages)
            sample_size = min(300, total_datasets)
            pages_to_fetch = min(3, (sample_size + 99) // 100)
            
            for page in range(1, pages_to_fetch + 1):
                params = {'page_size': 100, 'page': page, 'sort': '-created'}
                response = _self.session.get(DATASETS_URL, params=params, timeout=60)
                
                if response.status_code == 200:
                    data = response.json()
                    all_datasets.extend(data.get('data', []))
            
            if not all_datasets:
                return None
            
            # Calculer les m√©triques
            org_counts = {}
            format_counts = {}
            recent_count = 0
            total_views = 0
            
            for dataset in all_datasets:
                org = safe_get(dataset, ['organization', 'name'], 'Inconnue')
                org_counts[org] = org_counts.get(org, 0) + 1
                
                # Compter les formats
                for resource in dataset.get('resources', []):
                    fmt = (resource.get('format') or '').upper()
                    if fmt:
                        format_counts[fmt] = format_counts.get(fmt, 0) + 1
                
                # Datasets r√©cents
                if est_recent(convertir_date(dataset.get('created_at'))):
                    recent_count += 1
                
                # Total des vues
                total_views += dataset.get('metrics', {}).get('views', 0)
            
            avg_views = total_views // len(all_datasets) if all_datasets else 0
            
            return {
                'total_datasets': total_datasets,
                'organisations_count': len(org_counts),
                'top_organisations': dict(sorted(org_counts.items(), key=lambda x: x[1], reverse=True)[:10]),
                'top_formats': dict(sorted(format_counts.items(), key=lambda x: x[1], reverse=True)[:8]),
                'recent_datasets': recent_count,
                'avg_views': avg_views,
                'sample_size': len(all_datasets)
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur statistiques: {str(e)}")
            return None

# =============================================================================
# FONCTIONS DE DONN√âES EXTERNES
# =============================================================================
@st.cache_data(ttl=1800, show_spinner=False)
def charger_flux_rss():
    """Charge les flux RSS √©conomiques en temps r√©el"""
    articles = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:5]:
                articles.append({
                    'titre': entry.title,
                    'lien': entry.link,
                    'date': entry.get('published', 'Date inconnue'),
                    'source': feed.feed.get('title', 'Source inconnue'),
                    'resume': entry.get('summary', 'Aucun r√©sum√© disponible')
                })
        except Exception as e:
            continue
    
    return sorted(articles, key=lambda x: x['date'], reverse=True)[:15]

# =============================================================================
# COMPOSANTS D'INTERFACE UTILISATEUR
# =============================================================================
def afficher_sidebar(client):
    """Affiche la sidebar avec les contr√¥les de recherche"""
    with st.sidebar:
        st.markdown("""
        <div style='text-align: center; padding: 10px; background: linear-gradient(135deg, #0055A4 0%, #EF4135 100%); border-radius: 10px; margin-bottom: 20px;'>
            <h3 style='color: white; margin: 0;'>üá´üá∑ DATA.GOUV.FR</h3>
            <p style='color: white; margin: 0; font-size: 12px;'>Dashboard Analytics</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown('<div class="filter-section">', unsafe_allow_html=True)
        st.header("üîß RECHERCHE ET FILTRES")
        
        # Param√®tres de recherche
        st.subheader("üîç Crit√®res de Recherche")
        
        recherche_texte = st.text_input(
            "Mots-cl√©s",
            placeholder="√©conomie, finances, PIB, croissance...",
            help="Recherche dans les titres et descriptions des datasets"
        )
        
        # Suggestions de th√®mes
        st.subheader("üéØ Th√©matiques Pr√©d√©finies")
        theme_selection = st.selectbox(
            "Choisir une th√©matique",
            options=["Toutes les th√©matiques"] + list(THEMATIQUES.keys())
        )
        
        if theme_selection != "Toutes les th√©matiques":
            tags_suggestion = THEMATIQUES[theme_selection]
            st.info(f"**Tags sugg√©r√©s:** {', '.join(tags_suggestion[:3])}")
        
        # Organisation
        st.subheader("üè¢ Filtre par Organisation")
        with st.spinner("Chargement des organisations..."):
            organisations = client.get_organizations_list(50)
        noms_organisations = ["Toutes les organisations"] + [safe_get(org, ['name'], 'Inconnue') for org in organisations]
        organisation_selection = st.selectbox(
            "S√©lectionnez une organisation",
            options=noms_organisations
        )
        
        # Format de fichier
        st.subheader("üìÅ Format de Fichier")
        formats_fichiers = ["Tous les formats", "CSV", "XLS", "XLSX", "JSON", "PDF", "XML", "SHP", "GEOJSON"]
        format_selection = st.selectbox("S√©lectionnez un format", options=formats_fichiers)
        
        # Tags
        st.subheader("üè∑Ô∏è Tags Th√©matiques")
        tags_populaires = st.multiselect(
            "S√©lectionnez des tags",
            options=["√©conomie", "finances", "budget", "entreprises", "emploi", "commerce", 
                    "innovation", "statistiques", "fiscalit√©", "investissement", "donn√©es ouvertes",
                    "administration", "public", "gouvernement"],
            default=["√©conomie"]
        )
        
        # Options de recherche avanc√©e
        st.subheader("‚öôÔ∏è Options Avanc√©es")
        results_per_page = st.slider("R√©sultats par page", 10, 100, 20)
        only_recent = st.checkbox("üìÖ Datasets r√©cents seulement (30 derniers jours)", value=False)
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Statistiques rapides
        with st.spinner("Calcul des statistiques..."):
            stats = client.get_datasets_stats()
        
        if stats:
            st.markdown("---")
            st.subheader("üìä Statistiques Globales")
            st.write(f"**Total datasets:** {stats['total_datasets']:,}")
            st.write(f"**Organisations:** {stats['organisations_count']}")
            st.write(f"**√âchantillon analys√©:** {stats['sample_size']} datasets")
            st.write(f"**Formats principaux:** {', '.join(list(stats['top_formats'].keys())[:2])}")
    
    return {
        'recherche_texte': recherche_texte,
        'theme_selection': theme_selection,
        'organisation_selection': organisation_selection,
        'format_selection': format_selection,
        'tags_populaires': tags_populaires,
        'results_per_page': results_per_page,
        'only_recent': only_recent
    }

def afficher_resultat_dataset(dataset, i, current_page, client):
    """Affiche un r√©sultat de dataset de mani√®re organis√©e"""
    org_name = safe_get(dataset, ['organization', 'name'], 'Inconnue')
    created_at = (dataset.get('created_at') or '')[:10]
    metrics = dataset.get('metrics', {})
    is_recent = est_recent(convertir_date(dataset.get('created_at')))
    dataset_id = dataset.get('id', f'unknown_{i}')
    
    recent_badge = " üÜï" if is_recent else ""
    
    # Utiliser un conteneur avec style personnalis√©
    with st.container():
        st.markdown(f"""
        <div class="result-item">
            <div class="result-title">üìä {dataset.get('title', 'Sans titre')}{recent_badge}</div>
            <div class="result-meta">
                <strong>üè¢ Organisation:</strong> {org_name} | 
                <strong>üìÖ Publication:</strong> {created_at} | 
                <strong>üî¢ Ressources:</strong> {len(dataset.get('resources', []))} fichiers
            </div>
            <div class="result-stats">
                <strong>üëÅÔ∏è Visites:</strong> {metrics.get('views', 0):,} ‚Ä¢ 
                <strong>‚ù§Ô∏è Followers:</strong> {metrics.get('followers', 0)} ‚Ä¢ 
                <strong>üîÑ R√©utilisations:</strong> {metrics.get('reuses', 0)}
            </div>
            <div class="result-meta">
                <strong>üè∑Ô∏è Tags:</strong> {', '.join(dataset.get('tags', []))}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # Actions
        col_act1, col_act2, col_act3, col_act4 = st.columns([1, 1, 1, 1])
        
        with col_act1:
            # CORRECTION: Cl√© unique avec ID du dataset
            if st.button(f"üìà Analyser", key=f"analyze_{dataset_id}", use_container_width=True):
                st.session_state.dataset_analyse = dataset
                st.rerun()
        
        with col_act2:
            # CORRECTION: Cl√© unique avec ID du dataset
            if st.button(f"üìÅ D√©tails", key=f"details_{dataset_id}", use_container_width=True):
                st.session_state.dataset_details = dataset
                st.rerun()
        
        with col_act3:
            # CORRECTION: Cl√© unique avec ID du dataset
            if st.button(f"üì• M√©tadonn√©es", key=f"download_{dataset_id}", use_container_width=True):
                telecharger_dataset(dataset, client)
        
        with col_act4:
            dataset_url = f"https://www.data.gouv.fr/fr/datasets/{dataset.get('id', '')}/"
            st.markdown(
                f'<a href="{dataset_url}" target="_blank">'
                f'<button style="width: 100%; background: linear-gradient(135deg, #EF4135 0%, #FF6B6B 100%); color: white; padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; font-weight: bold;">'
                f'üåê Ouvrir</button></a>', 
                unsafe_allow_html=True
            )
        
        st.markdown("---")

def afficher_pagination_avancee(page_actuelle, total_pages, position="top"):
    """Affiche les contr√¥les de pagination avanc√©s"""
    if total_pages <= 1:
        return
        
    st.markdown("---")
    col_pag1, col_pag2, col_pag3, col_pag4, col_pag5 = st.columns([1, 2, 2, 2, 1])
    
    with col_pag1:
        if page_actuelle > 1:
            # CORRECTION: Cl√© unique avec num√©ro de page et position
            if st.button("‚¨ÖÔ∏è Pr√©c√©dent", key=f"prev_{page_actuelle}_{position}", use_container_width=True):
                st.session_state.recherche_page = page_actuelle - 1
                st.rerun()
    
    with col_pag2:
        pages_rapides = [1, max(1, page_actuelle-2), page_actuelle, min(total_pages, page_actuelle+2), total_pages]
        pages_rapides = sorted(set(pages_rapides))
        # CORRECTION: Cl√© unique avec num√©ro de page et position
        page_rapide = st.selectbox("Aller √† la page:", options=pages_rapides, index=pages_rapides.index(page_actuelle), key=f"page_select_{page_actuelle}_{position}")
        if page_rapide != page_actuelle:
            st.session_state.recherche_page = page_rapide
            st.rerun()
    
    with col_pag3:
        # CORRECTION: Cl√© unique avec num√©ro de page et position
        nouvelle_page = st.number_input(
            "Page sp√©cifique:",
            min_value=1,
            max_value=total_pages,
            value=page_actuelle,
            key=f"page_input_{page_actuelle}_{position}",
            label_visibility="collapsed"
        )
    
    with col_pag4:
        # CORRECTION: Cl√© unique avec num√©ro de page et position
        if st.button("üîç Aller", key=f"go_{page_actuelle}_{position}", use_container_width=True) and nouvelle_page != page_actuelle:
            st.session_state.recherche_page = nouvelle_page
            st.rerun()
    
    with col_pag5:
        if page_actuelle < total_pages:
            # CORRECTION: Cl√© unique avec num√©ro de page et position
            if st.button("Suivant ‚û°Ô∏è", key=f"next_{page_actuelle}_{position}", use_container_width=True):
                st.session_state.recherche_page = page_actuelle + 1
                st.rerun()

# =============================================================================
# FONCTIONS POUR LES GRAPHIQUES AVEC STYLES CORRIG√âS
# =============================================================================
def creer_graphique_organisations(stats):
    """Cr√©e le graphique des organisations avec styles corrig√©s"""
    if not stats['top_organisations']:
        return None
        
    org_df = pd.DataFrame({
        'Organisation': list(stats['top_organisations'].keys()),
        'Datasets': list(stats['top_organisations'].values())
    }).head(15)
    
    fig = px.bar(
        org_df,
        x='Datasets',
        y='Organisation',
        orientation='h',
        title="Top 15 des Organisations Productrices",
        color='Datasets',
        color_continuous_scale=['#0055A4', '#EF4135']
    )
    
    # CORRECTION DES STYLES POUR LA LISIBILIT√â
    fig.update_layout(
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(color='#000000', size=12),  # Texte en noir pour meilleure lisibilit√©
        title_font=dict(color='#0055A4', size=16),
        xaxis=dict(
            title_font=dict(color='#000000'),
            tickfont=dict(color='#000000'),
            gridcolor='#E0E0E0'
        ),
        yaxis=dict(
            title_font=dict(color='#000000'),
            tickfont=dict(color='#000000')
        ),
        coloraxis_colorbar=dict(
            title_font=dict(color='#000000'),
            tickfont=dict(color='#000000')
        )
    )
    
    return fig

def creer_graphique_formats(stats):
    """Cr√©e le graphique des formats avec styles corrig√©s"""
    if not stats['top_formats']:
        return None
        
    format_df = pd.DataFrame({
        'Format': list(stats['top_formats'].keys()),
        'Count': list(stats['top_formats'].values())
    })
    
    fig = px.pie(
        format_df,
        values='Count',
        names='Format',
        title="R√©partition des Formats de Fichiers",
        hole=0.4,
        color_discrete_sequence=['#0055A4', '#1f77b4', '#EF4135', '#FF6B6B', '#4CAF50', '#FF9800']
    )
    
    # CORRECTION DES STYLES POUR LA LISIBILIT√â
    fig.update_layout(
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(color='#000000', size=12),  # Texte en noir pour meilleure lisibilit√©
        title_font=dict(color='#0055A4', size=16),
        legend=dict(
            font=dict(color='#000000'),
            bgcolor='white',
            bordercolor='#0055A4',
            borderwidth=1
        )
    )
    
    # Am√©liorer la lisibilit√© des labels
    fig.update_traces(
        textfont=dict(color='#000000', size=10),
        textposition='inside'
    )
    
    return fig

def creer_graphique_temporel(stats):
    """Cr√©e le graphique temporel avec styles corrig√©s"""
    dates_simulees = pd.date_range(end=datetime.now(), periods=12, freq='M')
    creations_simulees = np.random.poisson(stats['sample_size'] // 12, 12) * np.linspace(0.8, 1.2, 12)
    
    fig = px.line(
        x=dates_simulees,
        y=creations_simulees,
        title="Volume de cr√©ations de datasets (estimation)",
        labels={'x': 'Mois', 'y': 'Nouveaux datasets'}
    )
    
    fig.update_traces(line=dict(color='#EF4135', width=3))
    
    # CORRECTION DES STYLES POUR LA LISIBILIT√â
    fig.update_layout(
        plot_bgcolor='white',
        paper_bgcolor='white',
        font=dict(color='#000000', size=12),  # Texte en noir pour meilleure lisibilit√©
        title_font=dict(color='#0055A4', size=16),
        xaxis=dict(
            title_font=dict(color='#000000'),
            tickfont=dict(color='#000000'),
            gridcolor='#E0E0E0'
        ),
        yaxis=dict(
            title_font=dict(color='#000000'),
            tickfont=dict(color='#000000'),
            gridcolor='#E0E0E0'
        )
    )
    
    return fig

# =============================================================================
# ONGLETS PRINCIPAUX
# =============================================================================
def afficher_onglet_recherche_avancee(client, filtres):
    """Affiche l'onglet de recherche avanc√©e"""
    st.markdown('<h2 class="section-header">üîç RECHERCHE AVANC√âE - 24K DATASETS</h2>', unsafe_allow_html=True)
    
    # Construction de la requ√™te avanc√©e
    query_parts = []
    
    if filtres['recherche_texte']:
        query_parts.append(filtres['recherche_texte'])
    
    if filtres['theme_selection'] != "Toutes les th√©matiques":
        query_parts.extend(THEMATIQUES[filtres['theme_selection']])
    
    if filtres['organisation_selection'] != "Toutes les organisations":
        query_parts.append(f'organization:"{filtres["organisation_selection"]}"')
    
    if filtres['format_selection'] != "Tous les formats":
        query_parts.append(f'format:{filtres["format_selection"]}')
    
    if filtres['tags_populaires']:
        for tag in filtres['tags_populaires']:
            query_parts.append(f'tag:{tag}')
    
    if filtres['only_recent']:
        query_parts.append('created:last-month')
    
    query_final = " ".join(query_parts) if query_parts else ""
    
    # Contr√¥les de recherche
    col_btn1, col_btn2, col_btn3, col_info = st.columns([2, 1, 1, 2])
    
    with col_btn1:
        # CORRECTION: Cl√© unique pour le bouton de recherche
        if st.button("üöÄ Lancer la Recherche Avanc√©e", key="search_launch", type="primary", use_container_width=True):
            st.session_state.recherche_query = query_final
            st.session_state.recherche_page = 1
            st.rerun()
    
    with col_btn2:
        # CORRECTION: Cl√© unique pour le bouton reset
        if st.button("üîÑ R√©initialiser", key="search_reset", use_container_width=True):
            for key in ['recherche_query', 'recherche_resultats', 'recherche_page']:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()
    
    with col_btn3:
        # CORRECTION: Cl√© unique pour le bouton export
        if st.button("üíæ Exporter R√©sultats", key="search_export", use_container_width=True):
            if 'recherche_resultats' in st.session_state:
                exporter_resultats(st.session_state.recherche_resultats)
    
    # Affichage de la requ√™te
    if query_final:
        st.info(f"**Requ√™te avanc√©e:** `{query_final}`")
    
    # Ex√©cution de la recherche
    if 'recherche_query' in st.session_state:
        executer_recherche_avancee(client, st.session_state.recherche_query, 
                                 st.session_state.get('recherche_page', 1), filtres['results_per_page'])
    else:
        afficher_guide_recherche_avancee()

def executer_recherche_avancee(client, query, page, page_size):
    """Ex√©cute la recherche avanc√©e sur data.gouv.fr"""
    with st.spinner(f"üîç Recherche en cours (page {page})..."):
        resultats = client.rechercher_datasets_avances(query, page, page_size)
    
    if resultats is not None:
        st.session_state.recherche_resultats = resultats
        afficher_resultats_avances(resultats, client, page, page_size)
    else:
        st.error("‚ùå Impossible d'ex√©cuter la recherche. V√©rifiez votre connexion.")

def afficher_resultats_avances(resultats, client, current_page, page_size):
    """Affiche les r√©sultats de la recherche avanc√©e"""
    datasets = resultats.get('data', [])
    total_results = resultats.get('total', 0)
    total_pages = max(1, (total_results + page_size - 1) // page_size)
    
    # En-t√™te des r√©sultats
    col_header1, col_header2, col_header3 = st.columns([3, 1, 1])
    
    with col_header1:
        st.subheader(f"üìã {total_results:,} Datasets Trouv√©s")
        st.caption(f"Page {current_page} sur {total_pages} ‚Ä¢ {len(datasets)} r√©sultats affich√©s")
    
    with col_header2:
        st.metric("Page Actuelle", f"{current_page}/{total_pages}")
    
    with col_header3:
        if total_results > 0:
            couverture = min((current_page * page_size) / total_results * 100, 100)
            st.metric("Couverture", f"{couverture:.1f}%")
    
    # Indicateur de performance
    if total_results > 1000:
        st.info(f"üéØ **Conseil:** Utilisez les filtres avanc√©s pour affiner votre recherche parmi les {total_results:,} r√©sultats")
    
    # Pagination en haut - CORRECTION: Ajout du param√®tre position
    if total_pages > 1:
        afficher_pagination_avancee(current_page, total_pages, "top")
    
    # Affichage des r√©sultats
    if not datasets:
        st.warning("""
        ## üéØ Aucun dataset trouv√©
        
        **Suggestions pour am√©liorer votre recherche parmi 24K datasets:**
        
        - ‚úÖ Utilisez les th√®mes pr√©d√©finis pour cibler votre recherche
        - ‚úÖ Combinez plusieurs filtres (organisation + format + tags)
        - ‚úÖ Essayez des synonymes ou termes plus g√©n√©riques
        - ‚úÖ Consultez les datasets populaires dans l'onglet d√©di√©
        - ‚úÖ R√©duisez le nombre de filtres si trop restrictifs
        """)
        return
    
    # Statistiques rapides sur les r√©sultats
    if datasets:
        orgs = [safe_get(d, ['organization', 'name'], 'Inconnue') for d in datasets]
        formats = list(set([(r.get('format') or '').upper() for d in datasets for r in d.get('resources', [])]))
        
        col_stats1, col_stats2, col_stats3 = st.columns(3)
        with col_stats1:
            st.metric("Organisations", len(set(orgs)))
        with col_stats2:
            st.metric("Formats", len(formats))
        with col_stats3:
            recent_count = sum(1 for d in datasets if est_recent(convertir_date(d.get('created_at'))))
            st.metric("R√©cents", recent_count)
    
    # Affichage d√©taill√© des r√©sultats
    for i, dataset in enumerate(datasets):
        afficher_resultat_dataset(dataset, i, current_page, client)
    
    # Pagination en bas - CORRECTION: Ajout du param√®tre position
    if total_pages > 1:
        afficher_pagination_avancee(current_page, total_pages, "bottom")

def afficher_onglet_analytics_globaux(client):
    """Affiche l'onglet des analytics globaux"""
    st.markdown('<h2 class="section-header">üìä ANALYTICS GLOBAUX - 24K DATASETS</h2>', unsafe_allow_html=True)
    
    with st.spinner('üîç Analyse des 24K datasets en cours...'):
        stats = client.get_datasets_stats()
        datasets_populaires = client.get_datasets_populaires(100)
    
    if not stats:
        st.error("""
        ‚ùå Impossible de charger les donn√©es pour l'analyse globale
        
        **Solutions possibles:**
        - V√©rifiez votre connexion Internet
        - L'API data.gouv.fr peut √™tre temporairement indisponible
        - R√©essayez dans quelques instants
        """)
        return
    
    if not datasets_populaires:
        st.warning("‚ö†Ô∏è Donn√©es limit√©es disponibles pour l'analyse")
        # Cr√©er des donn√©es de d√©monstration pour les datasets populaires
        datasets_populaires = []
    
    # KPI Principaux
    st.subheader("üìà Indicateurs Cl√©s")
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric("Total Datasets", f"{stats['total_datasets']:,}")
    with col2:
        st.metric("Organisations", stats['organisations_count'])
    with col3:
        st.metric("Datasets R√©cents", stats['recent_datasets'])
    with col4:
        st.metric("Visites Moyennes", f"{stats['avg_views']:,}")
    with col5:
        couverture_recente = (stats['recent_datasets'] / min(100, stats['sample_size'])) * 100
        st.metric("Fra√Æcheur", f"{couverture_recente:.1f}%")
    
    # Graphiques d'analyse
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        # Top organisations - UTILISATION DE LA FONCTION CORRIG√âE
        fig_org = creer_graphique_organisations(stats)
        if fig_org:
            st.plotly_chart(fig_org, use_container_width=True)
        else:
            st.info("üìä Aucune donn√©e d'organisation disponible")
    
    with col_chart2:
        # R√©partition des formats - UTILISATION DE LA FONCTION CORRIG√âE
        fig_format = creer_graphique_formats(stats)
        if fig_format:
            st.plotly_chart(fig_format, use_container_width=True)
        else:
            st.info("üìÅ Aucune donn√©e de format disponible")
    
    # Analyse temporelle - UTILISATION DE LA FONCTION CORRIG√âE
    st.subheader("üìÖ Analyse Temporelle")
    fig_temporel = creer_graphique_temporel(stats)
    if fig_temporel:
        st.plotly_chart(fig_temporel, use_container_width=True)
    
    # Tableau des datasets les plus populaires
    if datasets_populaires:
        st.subheader("üèÜ Top 25 des Datasets les Plus Populaires")
        
        datasets_tries = sorted(datasets_populaires, 
                              key=lambda x: x.get('metrics', {}).get('views', 0), 
                              reverse=True)[:25]
        
        data_table = []
        for i, dataset in enumerate(datasets_tries, 1):
            data_table.append({
                'Rank': i,
                'Titre': dataset.get('title', 'Sans titre')[:60] + '...' if len(dataset.get('title', '')) > 60 else dataset.get('title', 'Sans titre'),
                'Organisation': safe_get(dataset, ['organization', 'name'], 'Inconnue'),
                'Visites': f"{dataset.get('metrics', {}).get('views', 0):,}",
                'Followers': dataset.get('metrics', {}).get('followers', 0),
                'Ressources': len(dataset.get('resources', [])),
                'R√©cent': 'üÜï' if est_recent(convertir_date(dataset.get('created_at'))) else ''
            })
        
        df_table = pd.DataFrame(data_table)
        st.dataframe(df_table, use_container_width=True, height=400)
    else:
        st.info("‚≠ê Aucun dataset populaire disponible pour l'instant")

def afficher_onglet_top_datasets(client):
    """Affiche l'onglet des datasets populaires"""
    st.markdown('<h2 class="section-header">‚≠ê TOP 50 DES DATASETS POPULAIRES</h2>', unsafe_allow_html=True)
    
    with st.spinner('üîç Chargement des 50 datasets les plus populaires...'):
        datasets_populaires = client.get_datasets_populaires(50)
    
    if not datasets_populaires:
        st.error("""
        ‚ùå Impossible de charger les datasets populaires
        
        **Causes possibles:**
        - Probl√®me de connexion √† l'API data.gouv.fr
        - Donn√©es temporairement indisponibles
        - Essayez de rafra√Æchir la page
        """)
        return
    
    st.success(f"‚úÖ {len(datasets_populaires)} datasets populaires charg√©s")
    
    # Filtres pour les datasets populaires
    col_filter1, col_filter2 = st.columns(2)
    with col_filter1:
        organisations_uniques = list(set([safe_get(d, ['organization', 'name'], 'Inconnue') for d in datasets_populaires]))
        filter_org = st.selectbox("Filtrer par organisation", 
                                options=["Toutes"] + organisations_uniques)
    with col_filter2:
        min_views = st.slider("Visites minimum", 0, 10000, 0)
    
    # Appliquer les filtres
    datasets_filtres = datasets_populaires
    if filter_org != "Toutes":
        datasets_filtres = [d for d in datasets_filtres if safe_get(d, ['organization', 'name']) == filter_org]
    datasets_filtres = [d for d in datasets_filtres if d.get('metrics', {}).get('views', 0) >= min_views]
    
    if not datasets_filtres:
        st.warning("Aucun dataset ne correspond aux filtres s√©lectionn√©s")
        return
    
    for i, dataset in enumerate(datasets_filtres):
        dataset_id = dataset.get('id', f'pop_{i}')
        with st.expander(f"üèÜ {i+1}. {dataset.get('title', 'Sans titre')}", expanded=i<3):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                org_name = safe_get(dataset, ['organization', 'name'], 'Inconnue')
                metrics = dataset.get('metrics', {})
                is_recent = est_recent(convertir_date(dataset.get('created_at')))
                
                st.write(f"**Organisation:** {org_name}")
                st.write(f"**Description:** {(dataset.get('description') or 'Aucune description disponible')[:300]}...")
                st.write(f"**üìä M√©triques:** üëÅÔ∏è {metrics.get('views', 0):,} vues ‚Ä¢ ‚ù§Ô∏è {metrics.get('followers', 0)} followers ‚Ä¢ üîÑ {metrics.get('reuses', 0)} r√©utilisations")
                st.write(f"**üìÅ Ressources:** {len(dataset.get('resources', []))} fichiers")
                st.write(f"**üè∑Ô∏è Tags:** {', '.join(dataset.get('tags', []))}")
                if is_recent:
                    st.success("üÜï Dataset r√©cent (moins de 30 jours)")
            
            with col2:
                # CORRECTION: Cl√© unique avec ID du dataset
                if st.button(f"üîç Analyser", key=f"pop_analyze_{dataset_id}"):
                    st.session_state.dataset_analyse = dataset
                    st.rerun()
                
                dataset_url = f"https://www.data.gouv.fr/fr/datasets/{dataset.get('id', '')}/"
                st.markdown(f'<a href="{dataset_url}" target="_blank"><button style="background: linear-gradient(135deg, #0055A4 0%, #1f77b4 100%); color: white; padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; margin-top: 10px; width: 100%; font-weight: bold;">üåê Ouvrir sur data.gouv.fr</button></a>', unsafe_allow_html=True)
                
                # Bouton de t√©l√©chargement rapide
                # CORRECTION: Cl√© unique avec ID du dataset
                if st.button(f"üì• M√©tadonn√©es", key=f"pop_dl_{dataset_id}"):
                    telecharger_dataset(dataset, client)

# =============================================================================
# FONCTIONS DE GESTION DES DONN√âES
# =============================================================================
def telecharger_dataset(dataset, client):
    """T√©l√©charge et pr√©pare un dataset pour l'export"""
    with st.spinner("üì• Pr√©paration du t√©l√©chargement..."):
        # Cr√©er un DataFrame avec les m√©tadonn√©es
        data = {
            'Titre': [dataset.get('title')],
            'Organisation': [safe_get(dataset, ['organization', 'name'])],
            'Description': [dataset.get('description', '')[:500]],
            'Tags': [', '.join(dataset.get('tags', []))],
            'Date cr√©ation': [dataset.get('created_at')],
            'Visites': [dataset.get('metrics', {}).get('views', 0)],
            'Followers': [dataset.get('metrics', {}).get('followers', 0)],
            'Ressources': [len(dataset.get('resources', []))],
            'URL': [f"https://www.data.gouv.fr/fr/datasets/{dataset.get('id', '')}/"]
        }
        
        df = pd.DataFrame(data)
        csv = df.to_csv(index=False)
        
        st.download_button(
            label="üì• T√©l√©charger les m√©tadonn√©es",
            data=csv,
            file_name=f"metadata_{dataset.get('id', 'dataset')}.csv",
            mime="text/csv",
            use_container_width=True
        )

def exporter_resultats(resultats):
    """Exporte les r√©sultats de recherche"""
    datasets = resultats.get('data', [])
    if not datasets:
        st.warning("Aucun r√©sultat √† exporter")
        return
    
    # Pr√©parer les donn√©es pour l'export
    export_data = []
    for dataset in datasets:
        export_data.append({
            'Titre': dataset.get('title'),
            'Organisation': safe_get(dataset, ['organization', 'name']),
            'Description': (dataset.get('description') or '')[:200],
            'Tags': ', '.join(dataset.get('tags', [])),
            'Date cr√©ation': dataset.get('created_at'),
            'Visites': dataset.get('metrics', {}).get('views', 0),
            'Followers': dataset.get('metrics', {}).get('followers', 0),
            'Ressources': len(dataset.get('resources', [])),
            'URL': f"https://www.data.gouv.fr/fr/datasets/{dataset.get('id', '')}/"
        })
    
    df = pd.DataFrame(export_data)
    csv = df.to_csv(index=False)
    
    st.download_button(
        label="üíæ Exporter tous les r√©sultats (CSV)",
        data=csv,
        file_name=f"recherche_data_gouv_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
        mime="text/csv",
        use_container_width=True
    )

def afficher_guide_recherche_avancee():
    """Affiche le guide d'utilisation de la recherche avanc√©e"""
    st.markdown("""
    ## üéØ Guide de Recherche Avanc√©e - 24K Datasets
    
    ### üîç Strat√©gies de Recherche Efficace :
    
    **Pour naviguer parmi 24,000 datasets:**
    
    1. **üéØ Utilisez les th√®mes pr√©d√©finis** - Cat√©gories optimis√©es pour data.gouv.fr
    2. **üè¢ Filtrez par organisation** - INSEE, Minist√®res, Collectivit√©s...
    3. **üìÅ Sp√©cifiez le format** - CSV, Excel, JSON, G√©odonn√©es...
    4. **üè∑Ô∏è Combinez les tags** - √âconomie + Finances + Statistiques
    
    ### üí° Exemples de Recherches Avanc√©es :
    
    - **`√©conomie organization:"INSEE" format:CSV`** - Donn√©es √©conomiques INSEE en CSV
    - **`budget finances tag:public`** - Budgets et finances publiques
    - **`entreprises emploi`** - Donn√©es sur entreprises et emploi
    - **`transport format:SHP`** - Donn√©es g√©ospatiales transport
    
    ### üöÄ Fonctionnalit√©s Avanc√©es :
    
    - **Recherche en temps r√©el** sur l'ensemble du catalogue
    - **Filtres combin√©s** pour une pr√©cision maximale
    - **Pagination intelligente** pour naviguer rapidement
    - **Export des r√©sultats** en CSV
    - **Analyse de qualit√©** des datasets
    - **T√©l√©chargement direct** des ressources
    
    ### üìä Conseils Performance :
    
    - Commencez par une recherche large puis affinez
    - Utilisez les datasets populaires comme point de d√©part
    - V√©rifiez la date de mise √† jour pour la fra√Æcheur des donn√©es
    - Consultez les analytics pour identifier les producteurs fiables
    """)

# =============================================================================
# FONCTION PRINCIPALE
# =============================================================================
def main():
    # En-t√™te principal
    st.markdown("""
    <div style='text-align: center; background: linear-gradient(135deg, #0055A4 0%, #FFFFFF 50%, #EF4135 100%); padding: 20px; border-radius: 15px; margin-bottom: 20px;'>
        <h1 class="main-header">üîç DASHBOARD DATA.GOUV.FR</h1>
        <p style='color: #0055A4; font-size: 1.2rem; font-weight: bold;'>Analyse de 24,187 datasets ouverts fran√ßais</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown('<p class="data-source">üì° Source: API data.gouv.fr - Donn√©es ouvertes de la R√©publique Fran√ßaise</p>', unsafe_allow_html=True)
    
    # Initialisation du client API
    client = DataGouvAPIClient()
    
    # Sidebar avec contr√¥les de recherche
    filtres = afficher_sidebar(client)
    
    # Onglets principaux
    tab1, tab2, tab3 = st.tabs([
        "üîç Recherche Avanc√©e", 
        "üìä Analytics Globaux", 
        "‚≠ê Top Datasets"
    ])
    
    # Contenu des onglets
    with tab1:
        afficher_onglet_recherche_avancee(client, filtres)
    
    with tab2:
        afficher_onglet_analytics_globaux(client)
    
    with tab3:
        afficher_onglet_top_datasets(client)

# =============================================================================
# GESTION DES √âTATS ET EX√âCUTION
# =============================================================================
if __name__ == "__main__":
    main()